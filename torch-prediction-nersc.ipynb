{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Prediction with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import prefect\n",
    "from prefect import flow, task, get_run_logger\n",
    "from prefect_dask import DaskTaskRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving notices: ...working... done\n",
      "Channels:\n",
      " - conda-forge\n",
      " - defaults\n",
      "Platform: linux-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /global/common/software/nersc/pe/conda-envs/24.1.0/python-3.11/nersc-python\n",
      "\n",
      "  added / updated specs:\n",
      "    - pytorch-cpu\n",
      "    - torchvision\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    alsa-lib-1.2.12            |       h4ab18f5_0         543 KB  conda-forge\n",
      "    aws-c-auth-0.7.22          |       h96bc93b_2         103 KB  conda-forge\n",
      "    aws-c-cal-0.6.14           |       h88a6e22_1          46 KB  conda-forge\n",
      "    aws-c-common-0.9.19        |       h4ab18f5_0         221 KB  conda-forge\n",
      "    aws-c-compression-0.2.18   |       h83b837d_6          19 KB  conda-forge\n",
      "    aws-c-event-stream-0.4.2   |      ha47c788_12          53 KB  conda-forge\n",
      "    aws-c-http-0.8.1           |      h29d6fba_17         190 KB  conda-forge\n",
      "    aws-c-io-0.14.8            |       h21d4f22_5         154 KB  conda-forge\n",
      "    aws-c-mqtt-0.10.4          |       h759edc4_4         160 KB  conda-forge\n",
      "    aws-c-s3-0.5.9             |       h594631b_3         107 KB  conda-forge\n",
      "    aws-c-sdkutils-0.1.16      |       h83b837d_2          54 KB  conda-forge\n",
      "    aws-checksums-0.1.18       |       h83b837d_6          49 KB  conda-forge\n",
      "    aws-crt-cpp-0.26.9         |       he3a8b3b_0         332 KB  conda-forge\n",
      "    aws-sdk-cpp-1.11.329       |       hba8bd5f_3         3.5 MB  conda-forge\n",
      "    brotli-1.1.0               |       hd590300_1          19 KB  conda-forge\n",
      "    brotli-bin-1.1.0           |       hd590300_1          19 KB  conda-forge\n",
      "    brotli-python-1.1.0        |  py311hb755f60_1         343 KB  conda-forge\n",
      "    c-ares-1.32.2              |       h4bc722e_0         176 KB  conda-forge\n",
      "    ca-certificates-2024.7.4   |       hbcca054_0         151 KB  conda-forge\n",
      "    certifi-2024.7.4           |     pyhd8ed1ab_0         156 KB  conda-forge\n",
      "    expat-2.6.2                |       h59595ed_0         134 KB  conda-forge\n",
      "    filelock-3.15.4            |     pyhd8ed1ab_0          17 KB  conda-forge\n",
      "    glib-2.80.2                |       hf974151_0         586 KB  conda-forge\n",
      "    glib-tools-2.80.2          |       hb6ce0ca_0         112 KB  conda-forge\n",
      "    glog-0.7.1                 |       hbabe93e_0         140 KB  conda-forge\n",
      "    gst-plugins-base-1.24.4    |       h9ad1361_0         2.7 MB  conda-forge\n",
      "    gstreamer-1.24.4           |       haf2f30d_0         1.9 MB  conda-forge\n",
      "    libabseil-20240116.2       | cxx17_he02047a_1         1.2 MB  conda-forge\n",
      "    libarrow-16.1.0            |   hcb6531f_6_cpu         7.9 MB  conda-forge\n",
      "    libarrow-acero-16.1.0      |   hac33072_6_cpu         585 KB  conda-forge\n",
      "    libarrow-dataset-16.1.0    |   hac33072_6_cpu         566 KB  conda-forge\n",
      "    libarrow-substrait-16.1.0  |   h7e0c224_6_cpu         537 KB  conda-forge\n",
      "    libbrotlicommon-1.1.0      |       hd590300_1          68 KB  conda-forge\n",
      "    libbrotlidec-1.1.0         |       hd590300_1          32 KB  conda-forge\n",
      "    libbrotlienc-1.1.0         |       hd590300_1         276 KB  conda-forge\n",
      "    libclang-cpp15-15.0.7      |default_h127d8a8_5        16.4 MB  conda-forge\n",
      "    libcurl-8.8.0              |       hca28451_0         396 KB  conda-forge\n",
      "    libexpat-2.6.2             |       h59595ed_0          72 KB  conda-forge\n",
      "    libglib-2.80.2             |       hf974151_0         3.7 MB  conda-forge\n",
      "    libgoogle-cloud-2.24.0     |       h2736e30_0         1.2 MB  conda-forge\n",
      "    libgoogle-cloud-storage-2.24.0|       h3d9a0c8_0         735 KB  conda-forge\n",
      "    libgrpc-1.62.2             |       h15f2491_0         7.0 MB  conda-forge\n",
      "    libparquet-16.1.0          |   h6a7eafb_6_cpu         1.1 MB  conda-forge\n",
      "    libpng-1.6.43              |       h2797004_0         281 KB  conda-forge\n",
      "    libpq-16.3                 |       ha72fbe1_0         2.4 MB  conda-forge\n",
      "    libprotobuf-4.25.3         |       h08a7969_0         2.7 MB  conda-forge\n",
      "    libre2-11-2023.09.01       |       h5a48ba9_2         227 KB  conda-forge\n",
      "    libsqlite-3.46.0           |       hde9e2c9_0         845 KB  conda-forge\n",
      "    libthrift-0.19.0           |       hb90f79a_1         400 KB  conda-forge\n",
      "    libtorch-2.3.0             |cpu_mkl_h0bb0d08_101        47.6 MB  conda-forge\n",
      "    libuv-1.48.0               |       hd590300_0         879 KB  conda-forge\n",
      "    libxkbcommon-1.7.0         |       h662e7e4_0         580 KB  conda-forge\n",
      "    libxml2-2.12.7             |       hc051c1a_1         688 KB  conda-forge\n",
      "    mysql-common-8.3.0         |       hf1915f5_4         766 KB  conda-forge\n",
      "    mysql-connector-python-8.3.0|  py311ha6695c7_0         777 KB  conda-forge\n",
      "    mysql-libs-8.3.0           |       hca2cd23_4         1.5 MB  conda-forge\n",
      "    nss-3.100                  |       hca3bf56_0         2.0 MB  conda-forge\n",
      "    openssl-3.3.1              |       h4bc722e_2         2.8 MB  conda-forge\n",
      "    orc-2.0.1                  |       h17fec99_1        1006 KB  conda-forge\n",
      "    pcre2-10.43                |       hcad00b1_0         929 KB  conda-forge\n",
      "    protobuf-4.25.3            |  py311h7b78aeb_0         390 KB  conda-forge\n",
      "    pulseaudio-client-17.0     |       hb77b528_0         740 KB  conda-forge\n",
      "    pyarrow-16.1.0             |  py311h781c19f_1          26 KB  conda-forge\n",
      "    pyarrow-core-16.1.0        |py311h8e2c35d_1_cpu         4.3 MB  conda-forge\n",
      "    pytorch-2.3.0              |cpu_mkl_py311hcb16b95_101        31.9 MB  conda-forge\n",
      "    pytorch-cpu-2.3.0          |cpu_mkl_py311he2922ba_101          22 KB  conda-forge\n",
      "    qt-main-5.15.8             |      hc9dc06e_21        58.5 MB  conda-forge\n",
      "    re2-2023.09.01             |       h7f4b329_2          26 KB  conda-forge\n",
      "    s2n-1.4.15                 |       he19d79f_0         341 KB  conda-forge\n",
      "    sleef-3.6.1                |       h3400bea_1         975 KB  conda-forge\n",
      "    snappy-1.2.1               |       ha2e4443_0          41 KB  conda-forge\n",
      "    sqlite-3.39.2              |       h4ff8645_0         1.5 MB  conda-forge\n",
      "    torchvision-0.18.1         |cpu_py311hf0a5325_1         6.6 MB  conda-forge\n",
      "    xorg-libx11-1.8.9          |       h8ee46fc_0         809 KB  conda-forge\n",
      "    zstd-1.5.6                 |       ha6fb4c9_0         542 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:       226.2 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  filelock           conda-forge/noarch::filelock-3.15.4-pyhd8ed1ab_0 \n",
      "  libarrow-acero     conda-forge/linux-64::libarrow-acero-16.1.0-hac33072_6_cpu \n",
      "  libarrow-dataset   conda-forge/linux-64::libarrow-dataset-16.1.0-hac33072_6_cpu \n",
      "  libarrow-substrait conda-forge/linux-64::libarrow-substrait-16.1.0-h7e0c224_6_cpu \n",
      "  libclang-cpp15     conda-forge/linux-64::libclang-cpp15-15.0.7-default_h127d8a8_5 \n",
      "  libgoogle-cloud-s~ conda-forge/linux-64::libgoogle-cloud-storage-2.24.0-h3d9a0c8_0 \n",
      "  libparquet         conda-forge/linux-64::libparquet-16.1.0-h6a7eafb_6_cpu \n",
      "  libre2-11          conda-forge/linux-64::libre2-11-2023.09.01-h5a48ba9_2 \n",
      "  libtorch           conda-forge/linux-64::libtorch-2.3.0-cpu_mkl_h0bb0d08_101 \n",
      "  libuv              conda-forge/linux-64::libuv-1.48.0-hd590300_0 \n",
      "  pyarrow-core       conda-forge/linux-64::pyarrow-core-16.1.0-py311h8e2c35d_1_cpu \n",
      "  pytorch            conda-forge/linux-64::pytorch-2.3.0-cpu_mkl_py311hcb16b95_101 \n",
      "  pytorch-cpu        conda-forge/linux-64::pytorch-cpu-2.3.0-cpu_mkl_py311he2922ba_101 \n",
      "  sleef              conda-forge/linux-64::sleef-3.6.1-h3400bea_1 \n",
      "  torchvision        conda-forge/linux-64::torchvision-0.18.1-cpu_py311hf0a5325_1 \n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  alsa-lib                                1.2.10-hd590300_0 --> 1.2.12-h4ab18f5_0 \n",
      "  aws-c-auth                               0.7.3-h28f7589_1 --> 0.7.22-h96bc93b_2 \n",
      "  aws-c-cal                                0.6.1-hc309b26_1 --> 0.6.14-h88a6e22_1 \n",
      "  aws-c-common                             0.9.0-hd590300_0 --> 0.9.19-h4ab18f5_0 \n",
      "  aws-c-compression                       0.2.17-h4d4d85c_2 --> 0.2.18-h83b837d_6 \n",
      "  aws-c-event-stream                       0.3.1-h2e3709c_4 --> 0.4.2-ha47c788_12 \n",
      "  aws-c-http                              0.7.11-h00aa349_4 --> 0.8.1-h29d6fba_17 \n",
      "  aws-c-io                               0.13.32-he9a53bd_1 --> 0.14.8-h21d4f22_5 \n",
      "  aws-c-mqtt                               0.9.3-hb447be9_1 --> 0.10.4-h759edc4_4 \n",
      "  aws-c-s3                                0.3.14-hf3aad02_1 --> 0.5.9-h594631b_3 \n",
      "  aws-c-sdkutils                          0.1.12-h4d4d85c_1 --> 0.1.16-h83b837d_2 \n",
      "  aws-checksums                           0.1.17-h4d4d85c_1 --> 0.1.18-h83b837d_6 \n",
      "  aws-crt-cpp                             0.21.0-hb942446_5 --> 0.26.9-he3a8b3b_0 \n",
      "  aws-sdk-cpp                           1.10.57-h85b1a90_19 --> 1.11.329-hba8bd5f_3 \n",
      "  brotli                                   1.0.9-h166bdaf_9 --> 1.1.0-hd590300_1 \n",
      "  brotli-bin                               1.0.9-h166bdaf_9 --> 1.1.0-hd590300_1 \n",
      "  brotli-python                       1.0.9-py311ha362b79_9 --> 1.1.0-py311hb755f60_1 \n",
      "  c-ares                                  1.25.0-hd590300_0 --> 1.32.2-h4bc722e_0 \n",
      "  ca-certificates                     2023.11.17-hbcca054_0 --> 2024.7.4-hbcca054_0 \n",
      "  certifi                           2023.11.17-pyhd8ed1ab_0 --> 2024.7.4-pyhd8ed1ab_0 \n",
      "  expat                                    2.5.0-hcb278e6_1 --> 2.6.2-h59595ed_0 \n",
      "  glib                                    2.78.3-hfc55251_0 --> 2.80.2-hf974151_0 \n",
      "  glib-tools                              2.78.3-hfc55251_0 --> 2.80.2-hb6ce0ca_0 \n",
      "  glog                                     0.6.0-h6f12383_0 --> 0.7.1-hbabe93e_0 \n",
      "  gst-plugins-base                        1.22.8-h8e1006c_1 --> 1.24.4-h9ad1361_0 \n",
      "  gstreamer                               1.22.8-h98fc4e7_1 --> 1.24.4-haf2f30d_0 \n",
      "  libabseil                     20230125.3-cxx17_h59595ed_0 --> 20240116.2-cxx17_he02047a_1 \n",
      "  libarrow                            12.0.1-hb87d912_8_cpu --> 16.1.0-hcb6531f_6_cpu \n",
      "  libbrotlicommon                          1.0.9-h166bdaf_9 --> 1.1.0-hd590300_1 \n",
      "  libbrotlidec                             1.0.9-h166bdaf_9 --> 1.1.0-hd590300_1 \n",
      "  libbrotlienc                             1.0.9-h166bdaf_9 --> 1.1.0-hd590300_1 \n",
      "  libcurl                                  8.5.0-hca28451_0 --> 8.8.0-hca28451_0 \n",
      "  libexpat                                 2.5.0-hcb278e6_1 --> 2.6.2-h59595ed_0 \n",
      "  libglib                                 2.78.3-h783c2da_0 --> 2.80.2-hf974151_0 \n",
      "  libgoogle-cloud                         2.12.0-hac9eb74_1 --> 2.24.0-h2736e30_0 \n",
      "  libgrpc                                 1.54.3-hb20ce57_0 --> 1.62.2-h15f2491_0 \n",
      "  libpng                                  1.6.39-h753d276_0 --> 1.6.43-h2797004_0 \n",
      "  libpq                                     16.1-h33b98f1_7 --> 16.3-ha72fbe1_0 \n",
      "  libprotobuf                            3.21.12-hfc55251_2 --> 4.25.3-h08a7969_0 \n",
      "  libsqlite                               3.44.2-h2797004_0 --> 3.46.0-hde9e2c9_0 \n",
      "  libthrift                               0.18.1-h8fd135c_2 --> 0.19.0-hb90f79a_1 \n",
      "  libxkbcommon                             1.6.0-hd429924_1 --> 1.7.0-h662e7e4_0 \n",
      "  libxml2                                 2.12.3-h232c23b_0 --> 2.12.7-hc051c1a_1 \n",
      "  mysql-common                            8.0.33-hf1915f5_6 --> 8.3.0-hf1915f5_4 \n",
      "  mysql-connector-p~                 8.0.31-py311h0cf059c_2 --> 8.3.0-py311ha6695c7_0 \n",
      "  mysql-libs                              8.0.33-hca2cd23_6 --> 8.3.0-hca2cd23_4 \n",
      "  nss                                       3.96-h1d7d5a4_0 --> 3.100-hca3bf56_0 \n",
      "  openssl                                  3.2.0-hd590300_1 --> 3.3.1-h4bc722e_2 \n",
      "  orc                                      1.9.0-h2f23424_1 --> 2.0.1-h17fec99_1 \n",
      "  pcre2                                    10.42-hcad00b1_0 --> 10.43-hcad00b1_0 \n",
      "  protobuf                          4.21.12-py311hcafe171_0 --> 4.25.3-py311h7b78aeb_0 \n",
      "  pulseaudio-client                         16.1-hb77b528_5 --> 17.0-hb77b528_0 \n",
      "  pyarrow                        12.0.1-py311h39c9aba_8_cpu --> 16.1.0-py311h781c19f_1 \n",
      "  qt-main                                5.15.8-h450f30e_18 --> 5.15.8-hc9dc06e_21 \n",
      "  re2                                 2023.03.02-h8c504da_0 --> 2023.09.01-h7f4b329_2 \n",
      "  s2n                                     1.3.49-h06160fa_0 --> 1.4.15-he19d79f_0 \n",
      "  snappy                                  1.1.10-h9fff704_0 --> 1.2.1-ha2e4443_0 \n",
      "  xorg-libx11                              1.8.7-h8ee46fc_0 --> 1.8.9-h8ee46fc_0 \n",
      "  zstd                                     1.5.5-hfc55251_0 --> 1.5.6-ha6fb4c9_0 \n",
      "\n",
      "The following packages will be DOWNGRADED:\n",
      "\n",
      "  sqlite                                  3.44.2-h2c6b66d_0 --> 3.39.2-h4ff8645_0 \n",
      "\n",
      "\n",
      "Proceed ([y]/n)? "
     ]
    }
   ],
   "source": [
    "!conda install torchvision pytorch-cpu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example follows Torch's [transfer learning tutorial](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html). We will\n",
    "\n",
    "1. Finetune a pretrained convolutional neural network on a specific task (ants vs. bees).\n",
    "2. Use a Dask cluster for batch prediction with that model.\n",
    "\n",
    "_Note:_ The dependencies for this example are not installed by default in the Binder environment. You'll need to execute\n",
    "\n",
    "```\n",
    "!conda install torchvision pytorch-cpu\n",
    "```\n",
    "\n",
    "in a cell to install the necessary packages.\n",
    "\n",
    "The primary focus is using a Dask cluster for batch prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the data\n",
    "\n",
    "The PyTorch documentation hosts a small set of data. We'll download and extract it locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename, _ = urllib.request.urlretrieve(\"https://download.pytorch.org/tutorial/hymenoptera_data.zip\", \"data.zip\")\n",
    "zipfile.ZipFile(filename).extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The directory looks like\n",
    "\n",
    "```\n",
    "hymenoptera_data/\n",
    "    train/\n",
    "        ants/\n",
    "            0013035.jpg\n",
    "            ...\n",
    "            1030023514_aad5c608f9.jpg\n",
    "        bees/\n",
    "            1092977343_cb42b38d62.jpg\n",
    "            ...\n",
    "            2486729079_62df0920be.jpg\n",
    "     \n",
    "    train/\n",
    "        ants/\n",
    "            0013025.jpg\n",
    "            ...\n",
    "            1030023514_aad5c606d9.jpg\n",
    "        bees/\n",
    "            1092977343_cb42b38e62.jpg\n",
    "            ...\n",
    "            2486729079_62df0921be.jpg\n",
    "  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the [tutorial](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html), we'll finetune the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from tutorial_helper import (imshow, train_model, visualize_model,\n",
    "                             dataloaders, class_names, finetune_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune the model\n",
    "\n",
    "Our base model is resnet18. It predicts for 1,000 categories, while ours just predicts 2 (ants or bees). To make this model train quickly on examples.dask.org, we'll only use a couple of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = finetune_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things seem OK on a few random images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Prediction with Dask\n",
    "\n",
    "Now for the main topic: using a pretrained model for batch prediction on a Dask cluster.\n",
    "There are two main complications, that both deal with minimizing the amount of data\n",
    "moved around:\n",
    "\n",
    "1. **Loading the data on the workers.**. We'll use `dask.delayed` to load the data on\n",
    "   the workers, rather than loading it on the client and sending it to the workers.\n",
    "2. **PyTorch neural networks are large.** We don't want them in Dask task graphs, and we\n",
    "   only want to move them around once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distributed import Client\n",
    "\n",
    "client = Client(n_workers=2, threads_per_worker=2)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data on the workers\n",
    "\n",
    "First, we'll define a couple helpers to load the data and preprocess it for the neural network.\n",
    "We'll use `dask.delayed` here so that the execuation is lazy and happens on the cluster.\n",
    "See [the delayed example](../delayed.ipynb) for more on using `dask.delayed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import toolz\n",
    "import dask\n",
    "import dask.array as da\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "@dask.delayed\n",
    "def load(path, fs=__builtins__):\n",
    "    with fs.open(path, 'rb') as f:\n",
    "        img = Image.open(f).convert(\"RGB\")\n",
    "        return img\n",
    "\n",
    "\n",
    "@dask.delayed\n",
    "def transform(img):\n",
    "    trn = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    return trn(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objs = [load(x) for x in glob.glob(\"hymenoptera_data/val/*/*.jpg\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the data from cloud storage, say Amazon S3, you would use\n",
    "\n",
    "```python\n",
    "import s3fs\n",
    "\n",
    "fs = s3fs.S3FileSystem(...)\n",
    "objs = [load(x, fs=fs) for x in fs.glob(...)]\n",
    "```\n",
    "\n",
    "The PyTorch model expects tensors of a specific shape, so let's\n",
    "transform them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensors = [transform(x) for x in objs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the model expects batches of inputs, so let's stack a few together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = [dask.delayed(torch.stack)(batch)\n",
    "           for batch in toolz.partition_all(10, tensors)]\n",
    "batches[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll write a small `predict` helper to predict the output class (0 or 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def predict(batch, model):\n",
    "    with torch.no_grad():\n",
    "        out = model(batch)\n",
    "        _, predicted = torch.max(out, 1)\n",
    "        predicted = predicted.numpy()\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving the model around\n",
    "\n",
    "PyTorch neural networks are large, so we don't want to repeat it many times in our task graph (once per batch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "dask.utils.format_bytes(len(pickle.dumps(model)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, we'll also wrap the model itself in `dask.delayed`. This means the model only shows up once in the Dask graph.\n",
    "\n",
    "Additionally, since we performed fine-tuning in the above (and that runs on a GPU if its available), we should move the model back to the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmodel = dask.delayed(model.cpu()) # ensuring model is on the CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll use the (delayed) `predict` method to get our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [predict(batch, dmodel) for batch in batches]\n",
    "dask.visualize(predictions[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualization is a bit messy, but the large PyTorch model is the box that's an ancestor of both `predict` tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can do the computation, using the Dask cluster to do all the work. Because the dataset we're working with is small, it's safe to just use `dask.compute` to bring the results back to the local Client. For a larger dataset you would want to write to disk or cloud storage or continue processing the predictions on the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = dask.compute(*predictions)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This example showed how to do batch prediction on a set of images using PyTorch and Dask.\n",
    "We were careful to load data remotely on the cluster, and to serialize the large neural network only once."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NERSC Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "nbsphinx": {
   "execute": "never"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
